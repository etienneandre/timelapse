#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# Created      : 2025/11/08
# Last modified: 2025/11/09
# Author       : Étienne André
# Disclaimer   : first version obtained as a union of fragments generated by a generative AI chatbot

import os
import sys
import shutil
from datetime import datetime
from PIL import Image
from PIL.ExifTags import TAGS

import cv2
import numpy as np
import glob #, os
from tqdm import tqdm


repertoire_sortie = ""

# PARTIE 1: renommage

def extraire_date_exif(chemin_image):
    """Retourne la date/heure EXIF sous forme de datetime, ou None si introuvable."""
    try:
        with Image.open(chemin_image) as img:
            exif_data = img._getexif()
            if not exif_data:
                return None

            for tag, value in exif_data.items():
                tag_name = TAGS.get(tag, tag)
                if tag_name in ("DateTimeOriginal", "DateTime", "DateTimeDigitized"):
                    try:
                        return datetime.strptime(value, "%Y:%m:%d %H:%M:%S")
                    except Exception:
                        return None
        return None
    except Exception:
        return None


def main_renommage():
    # Vérification des arguments
    if len(sys.argv) < 2:
        print("Usage : python script.py <repertoire_entree> [prefixe] [repertoire_sortie]")
        sys.exit(1)

    repertoire_entree = sys.argv[1]
    prefixe = sys.argv[2] if len(sys.argv) >= 3 else "exif"

    global repertoire_sortie
    repertoire_sortie = sys.argv[3] if len(sys.argv) >= 4 else repertoire_entree.rstrip("/\\") + "-exif"

    if not os.path.isdir(repertoire_entree):
        print(f"Erreur : le répertoire d’entrée '{repertoire_entree}' n’existe pas.")
        sys.exit(1)

    print("\n=== Entrées ===")
    print(f"Répertoire source    : {repertoire_entree}")
    print(f"Préfixe              : {prefixe}")
    print(f"Répertoire de sortie : {repertoire_sortie}")

    os.makedirs(repertoire_sortie, exist_ok=True)

    # Extensions d’images prises en charge
    extensions_valides = {".jpg", ".jpeg", ".png", ".tiff", ".bmp", ".heic", ".webp"}

    nb_ok = 0
    nb_erreurs = 0

    for nom_fichier in os.listdir(repertoire_entree):
        chemin_entree = os.path.join(repertoire_entree, nom_fichier)

        if not os.path.isfile(chemin_entree):
            continue

        _, ext = os.path.splitext(nom_fichier)
        if ext.lower() not in extensions_valides:
            continue

        date_exif = extraire_date_exif(chemin_entree)

        if date_exif:
            nouveau_nom = f"{prefixe}-{date_exif.strftime('%Y-%m-%d-%H-%M')}{ext.lower()}"
            nb_ok += 1
            chemin_sortie = os.path.join(repertoire_sortie, nouveau_nom)
            # TODO: éventuellement vérifier ici que le fichier n'existe pas déjà, auquel cas, le renuméroter
            shutil.copy2(chemin_entree, chemin_sortie)

        else:
            print(f"⚠️ Erreur: pas de données EXIF valides pour '{nom_fichier}'")
            # NOTE: finalement, on ne copie pas le fichier, oust
            # nouveau_nom = f"erreur-{nom_fichier}"
            nb_erreurs += 1


    print("\n=== Résumé ===")
    print(f"Images converties correctement : {nb_ok}")
    print(f"Images en erreur               : {nb_erreurs}")
    print(f"Fichiers enregistrés dans      : {repertoire_sortie}")


# PARTIE 2: alignement


# === Réglages ===
# VIDEO_NAME = "timelapse.mp4"
# FPS = 24
MAX_FEATURES = 5000
RATIO_TEST = 0.75       # ratio pour knnMatch
MIN_MATCH_COUNT = 8     # nombre minimal de correspondances fiables pour homographie
N_AVG = 20   # nombre d’images utilisées pour calculer la moyenne

############################################################
# VERSION 1

def align_images_old(im, im_ref):
    """
    Aligne im sur im_ref.
    Retourne (im_aligned, H) où H est la transformation (3x3) appliquée à im.
    Si échec, renvoie im (non modifiée) et la matrice identité.
    """
    im1_gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)
    im2_gray = cv2.cvtColor(im_ref, cv2.COLOR_BGR2GRAY)

    # Détecteur / descripteur ORB
    orb = cv2.ORB_create(MAX_FEATURES)
    kp1, des1 = orb.detectAndCompute(im1_gray, None)
    kp2, des2 = orb.detectAndCompute(im2_gray, None)

    # Si pas de descripteurs, on renvoie l'image d'origine
    if des1 is None or des2 is None or len(kp1) < 4 or len(kp2) < 4:
        H = np.eye(3, dtype=np.float32)
        return im.copy(), H

    # Matcher BF Hamming (pour ORB)
    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)

    # knnMatch + ratio test
    knn_matches = bf.knnMatch(des1, des2, k=2)
    good_matches = []
    for m_n in knn_matches:
        if len(m_n) != 2:
            continue
        m, n = m_n
        if m.distance < RATIO_TEST * n.distance:
            good_matches.append(m)

    if len(good_matches) >= MIN_MATCH_COUNT:
        # Extraire points
        pts1 = np.float32([kp1[m.queryIdx].pt for m in good_matches])
        pts2 = np.float32([kp2[m.trainIdx].pt for m in good_matches])

        # Homographie robuste
        H, mask = cv2.findHomography(pts1, pts2, cv2.RANSAC, 5.0)
        if H is not None:
            h, w = im_ref.shape[:2]
            im1_reg = cv2.warpPerspective(im, H, (w, h))
            return im1_reg, H

    # Si pas assez de bonnes correspondances pour homographie, essayer une transformation affine partielle
    # On retente en utilisant estimation affine sur toutes les bonnes matches (même si moins)
    if len(good_matches) >= 4:
        pts1 = np.float32([kp1[m.queryIdx].pt for m in good_matches])
        pts2 = np.float32([kp2[m.trainIdx].pt for m in good_matches])
        M, inliers = cv2.estimateAffinePartial2D(pts1, pts2, method=cv2.RANSAC)
        if M is not None:
            # convertir affine 2x3 en 3x3
            H_aff = np.vstack([M, [0,0,1]])
            h, w = im_ref.shape[:2]
            im1_reg = cv2.warpAffine(im, M, (w, h))
            return im1_reg, H_aff

    # Sinon fallback: renvoyer image inchangée et identité
    return im.copy(), np.eye(3, dtype=np.float32)

# === Traitement ===
def main_alignement_old():
    IMG_DIR = repertoire_sortie
    OUTPUT_DIR = IMG_DIR + "-alignes"

    os.makedirs(OUTPUT_DIR, exist_ok=True)

    files = sorted(glob.glob(os.path.join(IMG_DIR, "*.jpg")))
    if not files:
        raise SystemExit("Aucune image trouvée dans " + IMG_DIR)


    print(f"[INFO] Alignement de {len(files)} images…")
    ref = cv2.imread(files[0])
    aligned_images = [(files[0], ref)]

    for f in tqdm(files[1:], desc="Alignement"):
        im = cv2.imread(f)
        aligned, _ = align_images_old(im, ref)
        aligned_images.append((f, aligned))

    # # Calcul du recadrage commun (utiliser logical_and.reduce pour robustesse)
    # print("[INFO] Calcul du cadrage commun…")
    # masks = [(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) > 0) for img in aligned_images]
    # common_mask = np.logical_and.reduce(masks).astype(np.uint8)
    # ys, xs = np.where(common_mask)
    # if ys.size == 0 or xs.size == 0:
    #     raise SystemExit("Intersection vide — les images sont trop décalées après alignement.")
    # ymin, ymax, xmin, xmax = ys.min(), ys.max(), xs.min(), xs.max()

    # === 4. Calcul du recadrage commun ===
    print("[INFO] Calcul du cadrage commun (zone non noire)...")
    masks = [(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) > 0) for _, img in aligned_images]
    common_mask = np.logical_and.reduce(masks).astype(np.uint8)
    ys, xs = np.where(common_mask)
    if ys.size == 0 or xs.size == 0:
        raise SystemExit("Intersection vide — les images sont trop décalées après alignement.")
    ymin, ymax, xmin, xmax = ys.min(), ys.max(), xs.min(), xs.max()

    # === 5. Sauvegarde des images alignées ===
    print("[INFO] Sauvegarde des images alignées...")
    for orig_path, img in aligned_images:
        filename = os.path.basename(orig_path)
        cropped = img[ymin:ymax+1, xmin:xmax+1]
        outpath = os.path.join(OUTPUT_DIR, filename)
        cv2.imwrite(outpath, cropped)

    # cropped_images = [img[ymin:ymax+1, xmin:xmax+1] for img in aligned_images]
    #
    # print("[INFO] Sauvegarde des images alignées…")
    # for i, img in enumerate(cropped_images):
    #     outpath = os.path.join(OUTPUT_DIR, f"frame_{i:04d}.jpg")
    #     cv2.imwrite(outpath, img)

    # print("[INFO] Création de la vidéo finale avec framerate " + FPS + "…")
    # os.system(f"ffmpeg -y -framerate {FPS} -i {OUTPUT_DIR}/frame_%04d.jpg -c:v libx264 -pix_fmt yuv420p {VIDEO_NAME}")
    # print(f"[OK] Vidéo créée : {VIDEO_NAME}")

############################################################
# VERSION 2

# === Traitement ===
def main_alignement():
    # global repertoire_sortie
    IMG_DIR = repertoire_sortie
    OUTPUT_DIR = IMG_DIR + "-alignes"

    os.makedirs(OUTPUT_DIR, exist_ok=True)

    files = sorted(glob.glob(os.path.join(IMG_DIR, "*.jpg")))
    if not files:
        raise SystemExit(f"Aucune image trouvée dans {IMG_DIR}")

    # === 1. Calcul de l’image de référence moyenne ===
    print(f"[INFO] Calcul de l’image moyenne à partir des {min(N_AVG, len(files))} premières images...")
    subset = files[:N_AVG]
    imgs = []
    for f in subset:
        im = cv2.imread(f).astype(np.float32)
        imgs.append(im)
    avg_ref = np.mean(imgs, axis=0).astype(np.uint8)
    ref_gray = cv2.cvtColor(avg_ref, cv2.COLOR_BGR2GRAY)

    # Sauvegarde optionnelle pour vérifier la référence
    cv2.imwrite(os.path.join(OUTPUT_DIR, "_reference_average.jpg"), avg_ref)

    # === 2. Fonction d’alignement (inchangée sauf ref passée en paramètre) ===
    def align_images(im, im_ref_gray):
        im1_gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)
        orb = cv2.ORB_create(MAX_FEATURES)
        kp1, des1 = orb.detectAndCompute(im1_gray, None)
        kp2, des2 = orb.detectAndCompute(im_ref_gray, None)

        if des1 is None or des2 is None or len(kp1) < 4 or len(kp2) < 4:
            return im.copy(), np.eye(3, dtype=np.float32)

        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)
        knn_matches = bf.knnMatch(des1, des2, k=2)
        good_matches = []
        for m_n in knn_matches:
            if len(m_n) != 2:
                continue
            m, n = m_n
            if m.distance < RATIO_TEST * n.distance:
                good_matches.append(m)

        if len(good_matches) >= MIN_MATCH_COUNT:
            pts1 = np.float32([kp1[m.queryIdx].pt for m in good_matches])
            pts2 = np.float32([kp2[m.trainIdx].pt for m in good_matches])
            H, mask = cv2.findHomography(pts1, pts2, cv2.RANSAC, 5.0)
            if H is not None:
                h, w = im_ref_gray.shape[:2]
                im1_reg = cv2.warpPerspective(im, H, (w, h))
                return im1_reg, H

        # Fallback affine
        if len(good_matches) >= 4:
            pts1 = np.float32([kp1[m.queryIdx].pt for m in good_matches])
            pts2 = np.float32([kp2[m.trainIdx].pt for m in good_matches])
            M, inliers = cv2.estimateAffinePartial2D(pts1, pts2, method=cv2.RANSAC)
            if M is not None:
                h, w = im_ref_gray.shape[:2]
                im1_reg = cv2.warpAffine(im, M, (w, h))
                H_aff = np.vstack([M, [0, 0, 1]])
                return im1_reg, H_aff

        # Si rien ne marche
        return im.copy(), np.eye(3, dtype=np.float32)

    # === 3. Alignement de toutes les images sur l’image moyenne ===
    print(f"[INFO] Alignement de {len(files)} images sur la référence moyenne...")
    aligned_images = []
    for f in tqdm(files, desc="Alignement"):
        im = cv2.imread(f)
        aligned, _ = align_images(im, ref_gray)
        aligned_images.append((f, aligned))

    # === 4. Calcul du recadrage commun ===
    print("[INFO] Calcul du cadrage commun (zone non noire)...")
    masks = [(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) > 0) for _, img in aligned_images]
    common_mask = np.logical_and.reduce(masks).astype(np.uint8)
    ys, xs = np.where(common_mask)
    if ys.size == 0 or xs.size == 0:
        raise SystemExit("Intersection vide — les images sont trop décalées après alignement.")
    ymin, ymax, xmin, xmax = ys.min(), ys.max(), xs.min(), xs.max()

    # === 5. Sauvegarde des images alignées ===
    print("[INFO] Sauvegarde des images alignées...")
    for orig_path, img in aligned_images:
        filename = os.path.basename(orig_path)
        cropped = img[ymin:ymax+1, xmin:xmax+1]
        outpath = os.path.join(OUTPUT_DIR, filename)
        cv2.imwrite(outpath, cropped)

############################################################
# VERSION 3

def reprojection_error(H, src_pts, dst_pts, affine=False):
    if affine:
        # src_pts: Nx2
        src_pts_h = np.hstack([src_pts, np.ones((len(src_pts), 1))])
        pred = (H @ src_pts_h.T).T  # Nx2
    else:
        src_pts_h = np.hstack([src_pts, np.ones((len(src_pts), 1))])
        pred_h = (H @ src_pts_h.T).T  # Nx3
        pred = pred_h[:, :2] / pred_h[:, 2:3]

    err = np.linalg.norm(pred - dst_pts, axis=1)
    return np.median(err)  # ou np.mean(err)


def align_images_3(base_img, img_to_align, fname, max_features=5000, good_match_percent=0.15):
    """Aligne img_to_align sur base_img via détection de points clés (ORB) et homographie."""
    # Convertir en niveaux de gris

    # im1_gray = cv2.cvtColor(base_img, cv2.COLOR_BGR2GRAY)
    # im2_gray = cv2.cvtColor(img_to_align, cv2.COLOR_BGR2GRAY)

    # Mieux ?
    im1_gray = cv2.equalizeHist(cv2.cvtColor(base_img, cv2.COLOR_BGR2GRAY))
    im2_gray = cv2.equalizeHist(cv2.cvtColor(img_to_align, cv2.COLOR_BGR2GRAY))

    # Détection des points clés et descripteurs
    # BEGIN ORB
    # orb = cv2.ORB_create(max_features)
    # keypoints1, descriptors1 = orb.detectAndCompute(im1_gray, None)
    # keypoints2, descriptors2 = orb.detectAndCompute(im2_gray, None)
    # # Appariement des descripteurs
    # matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)
    # END ORB
    # BEGIN AKAZE
    # detector = cv2.AKAZE_create()  # ou cv2.SIFT_create()
    # keypoints1, descriptors1 = detector.detectAndCompute(im1_gray, None)
    # keypoints2, descriptors2 = detector.detectAndCompute(im2_gray, None)
    #
    # # Appariement des descripteurs
    # matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)
    # END AKAZE

    detector = cv2.SIFT_create()
    keypoints1, descriptors1 = detector.detectAndCompute(im1_gray, None)
    keypoints2, descriptors2 = detector.detectAndCompute(im2_gray, None)

    # Appariement des descripteurs
    matcher = cv2.DescriptorMatcher_create(cv2.DescriptorMatcher_FLANNBASED)

    # BEGIN SIMPLE COMPARISON
    # # matches = matcher.match(descriptors1, descriptors2, None)
    # matches = list(matcher.match(descriptors1, descriptors2))
    #
    # # Trier selon la qualité (distance)
    # matches.sort(key=lambda x: x.distance, reverse=False)
    #
    # # Garder les meilleures correspondances
    # num_good_matches = int(len(matches) * good_match_percent)
    # matches = matches[:num_good_matches]
    # END SIMPLE COMPARISON

    # Appariement avec knn
    potential_matches = matcher.knnMatch(descriptors1, descriptors2, k=2)

    matches = []
    for m, n in potential_matches:
        if m.distance < 0.75 * n.distance:
            matches.append(m)

    # Extraire les points correspondants
    points1 = np.zeros((len(matches), 2), dtype=np.float32)
    points2 = np.zeros((len(matches), 2), dtype=np.float32)
    for i, match in enumerate(matches):
        points1[i, :] = keypoints1[match.queryIdx].pt
        points2[i, :] = keypoints2[match.trainIdx].pt

    height, width, channels = base_img.shape

    # Calcul de la transformation par homographie
    h_homog, mask_h = cv2.findHomography(points2, points1, cv2.RANSAC)

    # Calcul de la transformation affine
    h_affine, mask_a = cv2.estimateAffinePartial2D(points2, points1, method=cv2.RANSAC)

    err_homog = reprojection_error(h_homog, points2, points1, affine=False)
    err_affine = reprojection_error(h_affine, points2, points1, affine=True)

    if err_affine < err_homog:
        use_affine = True
    else:
        use_affine = False

    # if h is None or np.linalg.cond(h) > 1e4:
    #     h, mask = cv2.estimateAffinePartial2D(points2, points1, method=cv2.RANSAC)
    #     if h is not None:
    #         print(f"⚠️ Homographie douteuse pour {fname}, backup vers translation.")
    #         aligned_image = cv2.warpAffine(img_to_align, h, (width, height))
    #     else:
    #         print(f"⚠️ Homographie et translation douteuses pour {fname}, copie non alignée.")
    #         aligned_image = img_to_align.copy()
    # else:
    #     # Appliquer la transformation homographie
    #     aligned_image = cv2.warpPerspective(img_to_align, h, (width, height))

    h = h_affine

    if use_affine:
        # Appliquer la transformation affine
        aligned_image = cv2.warpAffine(img_to_align, h_affine, (width, height))
        h = h_affine
    else:
        # Appliquer la transformation homographie
        aligned_image = cv2.warpPerspective(img_to_align, h_homog, (width, height))
        h = h_homog
        print(f"⚠️ Meilleure homographie pour {fname}.")

    return aligned_image, h

def align_folder_3(input_dir, output_dir):
    os.makedirs(output_dir, exist_ok=True)
    filenames = sorted([
        f for f in os.listdir(input_dir)
        if f.lower().endswith((".jpg", ".jpeg", ".png", ".tif"))
    ])

    if not filenames:
        print("Aucune image trouvée dans", input_dir)
        return

    # Image de référence
    ref_path = os.path.join(input_dir, filenames[0])
    ref_img = cv2.imread(ref_path)
    if ref_img is None:
        raise ValueError(f"Impossible de lire l'image de référence : {ref_path}")

    print(f"Image de référence : {filenames[0]}")

    # Sauvegarder l'image de référence telle quelle
    cv2.imwrite(os.path.join(output_dir, filenames[0]), ref_img)

    # Aligner les autres
    for fname in tqdm(filenames[1:], desc="Alignement des images"):
        path = os.path.join(input_dir, fname)
        img = cv2.imread(path)
        if img is None:
            print(f"⚠️  Impossible de lire {fname}, ignorée.")
            continue
        aligned, _ = align_images_3(ref_img, img, fname)
        cv2.imwrite(os.path.join(output_dir, fname), aligned)

    print(f"\n✅ Toutes les images ont été alignées dans : {output_dir}")

def main_alignement_3():
    IMG_DIR = repertoire_sortie
    OUTPUT_DIR = IMG_DIR + "-alignes"

    # import argparse
    # parser = argparse.ArgumentParser(description="Aligne les images d'un dossier pour timelapse.")
    # parser.add_argument("input_dir", help="Dossier contenant les images originales")
    # parser.add_argument("output_dir", help="Dossier où sauvegarder les images alignées")
    # args = parser.parse_args()
    # align_folder(args.input_dir, args.output_dir)
    align_folder_3(IMG_DIR, OUTPUT_DIR)

############################################################
# MAIN

if __name__ == "__main__":
    main_renommage()
    # main_alignement_old()
    # main_alignement()
    main_alignement_3()

print("Done! :-)")
