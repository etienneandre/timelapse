#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# Created      : 2025/11/08
# Last modified: 2025/11/12
# Author       : √âtienne Andr√©
# Disclaimer   : first version obtained as a union of fragments generated by (and subsequently improved by incremental requests to) a generative AI chatbot

from enum import Enum
from collections import Counter

import os
import sys
import subprocess
import re
import shutil
from datetime import datetime
from PIL import Image, ImageDraw, ImageFont
from PIL.ExifTags import TAGS

import cv2
import numpy as np
import glob #, os
from tqdm import tqdm


from skimage.metrics import structural_similarity as ssim

############################################################
# Constants
############################################################
# Generate all possible alignments
DEBUG_ALL_IMAGES = True
# Print alignment methods for each image (with error)
DEBUG_METHODS = False
PREFIX_DEBUG_IMAGES = 'LOG-'

VIDEO_NAME = "timelapse.mp4"
# FPS = 4
DURATION_FRAME = "0.1"

# ERROR_THRESHOLD = 20

# largeur maximale en pixels lors du redimensionnement ; use `None` for no resizing
MAX_LARGEUR = 400
# MAX_LARGEUR = None

# NOTE: completely ad-hoc‚Ä¶! (for mostly day pictures)
# ALIGNMENT_THRESHOLD = 0.75
# NOTE: completely ad-hoc‚Ä¶! (for pictures including nights)
ALIGNMENT_THRESHOLD = 0.83

############################################################
# Enum
############################################################

class AlignmentMethod(Enum):
    # HOMOGRAPHY = "Homography"
    # AFFINE     = "Affine"
    # FALLBACK_AFFINE = "affine (fallback)"
    ERROR      = "error"
    ALIGN_ORB      = "align_orb"
    # ALIGN_ECC      = "align_ecc"
    # ALIGN_PHASE_CORRELATION      = "align_phase_correlation"
    ALIGN_AFFINE_AKAZE = "align_affine_akaze"
    ALIGN_AFFINE_SIFT = "align_affine_sift"
    ALIGN_AFFINE_ORB = "align_affine_orb"


############################################################
# PARTIE 1: renommage et redimensionnement
############################################################

# TODO: detect if 2 pictures have the same %Y-%m-%d-%H-%M

def add_suffix(output_dir, fname, suffix):
    base, ext = os.path.splitext(fname)
    new_fname = f"{base}{suffix}{ext}"
    return os.path.join(output_dir, new_fname)

def extraire_date_exif(chemin_image):
    """Retourne la date/heure EXIF sous forme de datetime, ou None si introuvable."""
    try:
        with Image.open(chemin_image) as img:
            exif_data = img._getexif()
            if not exif_data:
                return None

            for tag, value in exif_data.items():
                tag_name = TAGS.get(tag, tag)
                if tag_name in ("DateTimeOriginal", "DateTime", "DateTimeDigitized"):
                    try:
                        return datetime.strptime(value, "%Y:%m:%d %H:%M:%S")
                    except Exception:
                        return None
        return None
    except Exception:
        return None


def redimensionner_image(chemin_entree, chemin_sortie, max_largeur):
    """Redimensionne l'image si elle d√©passe la largeur maximale, sinon la copie telle quelle."""
    if max_largeur is None:
        shutil.copy2(chemin_entree, chemin_sortie)
    else:
        try:
            with Image.open(chemin_entree) as img:
                largeur, hauteur = img.size
                if largeur > max_largeur:
                    ratio = max_largeur / largeur
                    nouvelle_hauteur = int(hauteur * ratio)
                    img = img.resize((max_largeur, nouvelle_hauteur), Image.LANCZOS)
                    img.save(chemin_sortie)
                else:
                    shutil.copy2(chemin_entree, chemin_sortie)
        except Exception as e:
            print(f"[ERREUR] Redimensionnement √©chou√© pour {chemin_entree} : {e}")
            shutil.copy2(chemin_entree, chemin_sortie)


def rename_after_exif(repertoire_entree, repertoire_sortie):
    # Extensions d‚Äôimages prises en charge
    extensions_valides = {".jpg", ".jpeg", ".png", ".tiff", ".bmp", ".heic", ".webp"}

    nb_ok = 0
    nb_erreurs = 0

    for nom_fichier in os.listdir(repertoire_entree):
        chemin_entree = os.path.join(repertoire_entree, nom_fichier)

        if not os.path.isfile(chemin_entree):
            continue

        _, ext = os.path.splitext(nom_fichier)
        if ext.lower() not in extensions_valides:
            continue

        date_exif = extraire_date_exif(chemin_entree)

        if date_exif:
            nouveau_nom = f"{prefixe}-{date_exif.strftime('%Y-%m-%d-%H-%M')}{ext.lower()}"
            nb_ok += 1
            chemin_sortie = os.path.join(repertoire_sortie, nouveau_nom)
            # TODO: √©ventuellement v√©rifier ici que le fichier n'existe pas d√©j√†, auquel cas, le renum√©roter
            # shutil.copy2(chemin_entree, chemin_sortie)
            redimensionner_image(chemin_entree, chemin_sortie, MAX_LARGEUR)

        else:
            print(f"‚ö†Ô∏è Erreur: pas de donn√©es EXIF valides pour '{nom_fichier}'")
            # NOTE: finalement, on ne copie pas le fichier, oust
            # nouveau_nom = f"erreur-{nom_fichier}"
            nb_erreurs += 1


    print("\n=== R√©sum√© du renommage et redimensionnement ===")
    print(f"Images converties correctement : {nb_ok}")
    print(f"Images en erreur               : {nb_erreurs}")
    print(f"Fichiers enregistr√©s dans      : {repertoire_sortie}")


############################################################
# PART 2: ALIGNMENT
############################################################

max_features = 5000            # pour ORB
good_match_percent = 0.15      # pour filtrer les correspondances
# ref_image_index = 0            # 0 = premi√®re image comme r√©f√©rence

def normalize_brightness(img):
    """Normalise la luminosit√© pour r√©duire l‚Äôimpact jour/nuit."""
    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    img_eq = cv2.equalizeHist(img_gray)
    return img_eq

# def alignment_error(im1, im2):
#     """Retourne une erreur d'alignement robuste √† la lumi√®re."""
#     im1_gray = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)
#     im2_gray = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)
#
#     # Normalisation sur [0, 1]
#     im1n = cv2.normalize(im1_gray.astype(np.float32), None, 0, 1, cv2.NORM_MINMAX)
#     im2n = cv2.normalize(im2_gray.astype(np.float32), None, 0, 1, cv2.NORM_MINMAX)
#
#     # SSIM (structure)
#     ssim_score = ssim(im1n, im2n, data_range=1.0)
#
#     # Normalized cross-correlation
#     mean1, mean2 = np.mean(im1n), np.mean(im2n)
#     numerator = np.sum((im1n - mean1) * (im2n - mean2))
#     denominator = np.sqrt(np.sum((im1n - mean1) ** 2) * np.sum((im2n - mean2) ** 2))
#     ncc = numerator / (denominator + 1e-8)
#
#     # Erreur combin√©e (0 = parfait, 1 = mauvais)
#     error = 0.6 * (1 - ncc) + 0.4 * (1 - ssim_score)
#     return error

def alignment_error_light_intensity(im1, im2):
    """Erreur robuste combinant NCC, SSIM et coh√©rence de gradient."""
    gray1 = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)
    gray2 = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)
    gray1 = cv2.normalize(gray1.astype(np.float32), None, 0, 1, cv2.NORM_MINMAX)
    gray2 = cv2.normalize(gray2.astype(np.float32), None, 0, 1, cv2.NORM_MINMAX)

    # Masque : on ignore les zones tr√®s sombres ou tr√®s claires
    mask = ((gray1 > 0.1) & (gray1 < 0.9) & (gray2 > 0.1) & (gray2 < 0.9)).astype(np.float32)

    # SSIM local (structure)
    ssim_score = ssim(gray1, gray2, data_range=1.0)

    # Corr√©lation normalis√©e (NCC)
    mean1, mean2 = np.mean(gray1[mask > 0]), np.mean(gray2[mask > 0])
    num = np.sum((gray1 - mean1) * (gray2 - mean2) * mask)
    den = np.sqrt(np.sum((gray1 - mean1)**2 * mask) * np.sum((gray2 - mean2)**2 * mask))
    ncc = num / (den + 1e-8)

    # Coh√©rence de gradient (contours align√©s)
    g1x, g1y = cv2.Sobel(gray1, cv2.CV_32F, 1, 0), cv2.Sobel(gray1, cv2.CV_32F, 0, 1)
    g2x, g2y = cv2.Sobel(gray2, cv2.CV_32F, 1, 0), cv2.Sobel(gray2, cv2.CV_32F, 0, 1)
    grad_diff = np.mean(np.abs(g1x - g2x) + np.abs(g1y - g2y))

    # Combinaison pond√©r√©e
    ncc = np.clip(ncc, -1, 1)
    ssim_loss = 1 - ssim_score
    ncc_loss = 1 - ncc
    grad_loss = np.clip(grad_diff, 0, 1)

    error = 0.4 * ssim_loss + 0.4 * ncc_loss + 0.2 * grad_loss
    return float(np.clip(error, 0, 1))

def alignment_error_light_invariant(im1, im2):
    """Calcule une erreur d'alignement robuste aux changements de lumi√®re (jour/nuit)."""
    gray1 = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)
    gray2 = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)

    # --- 1. Normalisation locale de contraste (invariance photom√©trique)
    gray1 = cv2.equalizeHist(gray1)
    gray2 = cv2.equalizeHist(gray2)

    # --- 2. D√©tection des contours (invariance forte √† la lumi√®re)
    edges1 = cv2.Canny(gray1, 50, 150)
    edges2 = cv2.Canny(gray2, 50, 150)

    # --- 3. Similarit√© de contours (Jaccard ou Dice)
    inter = np.logical_and(edges1 > 0, edges2 > 0).sum()
    union = np.logical_or(edges1 > 0, edges2 > 0).sum()
    if union == 0:
        edge_sim = 0
    else:
        edge_sim = inter / union  # entre 0 et 1

    # --- 4. Corr√©lation des gradients
    g1x, g1y = cv2.Sobel(gray1, cv2.CV_32F, 1, 0), cv2.Sobel(gray1, cv2.CV_32F, 0, 1)
    g2x, g2y = cv2.Sobel(gray2, cv2.CV_32F, 1, 0), cv2.Sobel(gray2, cv2.CV_32F, 0, 1)
    mag1 = np.sqrt(g1x**2 + g1y**2)
    mag2 = np.sqrt(g2x**2 + g2y**2)
    grad_corr = np.corrcoef(mag1.flatten(), mag2.flatten())[0, 1]
    grad_corr = np.nan_to_num(grad_corr, nan=0.0)
    grad_corr = np.clip(grad_corr, 0, 1)

    # --- 5. SSIM sur images localement normalis√©es (faible pond√©ration)
    gray1n = cv2.normalize(gray1.astype(np.float32), None, 0, 1, cv2.NORM_MINMAX)
    gray2n = cv2.normalize(gray2.astype(np.float32), None, 0, 1, cv2.NORM_MINMAX)
    ssim_score = ssim(gray1n, gray2n, data_range=1.0)

    # --- 6. Combinaison pond√©r√©e (priorit√© aux gradients et contours)
    score = 0.5 * edge_sim + 0.4 * grad_corr + 0.1 * ssim_score

    # Convertir en "erreur" (0 = parfait, 1 = mauvais)
    error = 1 - np.clip(score, 0, 1)
    return float(error)

def alignment_error(im1, im2):
    error_geom = alignment_error_light_invariant(im1, im2)
    error_photo = alignment_error_light_intensity(im1, im2)
    error = 0.8 * error_geom + 0.2 * error_photo
    return error


def align_orb(im1, im2):
    """Alignement bas√© sur ORB + homographie"""
    im1_gray = normalize_brightness(im1)
    im2_gray = normalize_brightness(im2)

    orb = cv2.ORB_create(max_features)
    keypoints1, descriptors1 = orb.detectAndCompute(im1_gray, None)
    keypoints2, descriptors2 = orb.detectAndCompute(im2_gray, None)

    if descriptors1 is None or descriptors2 is None:
        return None, np.inf

    matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)

    # matches = matcher.match(descriptors1, descriptors2, None)
    # matches.sort(key=lambda x: x.distance, reverse=False)

    matches = matcher.match(descriptors1, descriptors2)

    # Uniformiser le format : forcer √† une liste
    if isinstance(matches, (tuple, set)):
        matches = list(matches)
    elif isinstance(matches, cv2.DMatch):
        matches = [matches]

    matches = sorted(matches, key=lambda x: x.distance)

    if len(matches) < 4:
        return None, np.inf  # pas assez de points pour une homographie

    num_good_matches = int(len(matches) * good_match_percent)
    matches = matches[:num_good_matches]

    pts1 = np.zeros((len(matches), 2), dtype=np.float32)
    pts2 = np.zeros_like(pts1)
    for i, m in enumerate(matches):
        pts1[i, :] = keypoints1[m.queryIdx].pt
        pts2[i, :] = keypoints2[m.trainIdx].pt

    H, mask = cv2.findHomography(pts2, pts1, cv2.RANSAC)
    if H is None:
        return None, np.inf

    height, width, _ = im1.shape
    im2_aligned = cv2.warpPerspective(im2, H, (width, height))
    # error = 1 - ssim(cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY), cv2.cvtColor(im2_aligned, cv2.COLOR_BGR2GRAY))
    error = alignment_error(im1, im2_aligned)
    return im2_aligned, error


def align_ecc(im1, im2):
    """Alignement bas√© sur l‚ÄôECC (Enhanced Correlation Coefficient)"""
    im1_gray = normalize_brightness(im1)
    im2_gray = normalize_brightness(im2)

    warp_matrix = np.eye(2, 3, dtype=np.float32)
    criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 2000, 1e-7)
    try:
        cc, warp_matrix = cv2.findTransformECC(im1_gray, im2_gray, warp_matrix, cv2.MOTION_AFFINE, criteria)
        aligned = cv2.warpAffine(im2, warp_matrix, (im1.shape[1], im1.shape[0]),
                                 flags=cv2.INTER_LINEAR + cv2.WARP_INVERSE_MAP)
        # error = 1 - cc
        error = alignment_error(im1, aligned)
        return aligned, error
    except cv2.error:
        return None, np.inf


# def align_phase_correlation(im1, im2):
#     """Alignement translation simple par corr√©lation de phase."""
#     im1_gray = normalize_brightness(im1)
#     im2_gray = normalize_brightness(im2)
#     shift = cv2.phaseCorrelate(np.float32(im1_gray), np.float32(im2_gray))[0]
#     dx, dy = shift
#     M = np.float32([[1, 0, dx], [0, 1, dy]])
#     aligned = cv2.warpAffine(im2, M, (im1.shape[1], im1.shape[0]))
#     error = 1 - ssim(cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY),
#                      cv2.cvtColor(aligned, cv2.COLOR_BGR2GRAY))
#     return aligned, error
def align_phase_correlation(im1, im2):
    """Alignement translation simple par corr√©lation de phase."""
    im1_gray = normalize_brightness(im1)
    im2_gray = normalize_brightness(im2)

    # V√©rifie et ajuste la taille
    if im1_gray.shape != im2_gray.shape:
        im2_gray = cv2.resize(im2_gray, (im1_gray.shape[1], im1_gray.shape[0]))

    try:
        shift = cv2.phaseCorrelate(np.float32(im1_gray), np.float32(im2_gray))[0]
    except cv2.error as e:
        # s√©curit√© suppl√©mentaire : si phaseCorrelate √©choue
        print(f"[‚ö†Ô∏è] phaseCorrelate a √©chou√© : {e}")
        return None, np.inf

    dx, dy = shift
    M = np.float32([[1, 0, dx], [0, 1, dy]])
    aligned = cv2.warpAffine(im2, M, (im1.shape[1], im1.shape[0]))
    # error = 1 - ssim(cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY), cv2.cvtColor(aligned, cv2.COLOR_BGR2GRAY))
    error = alignment_error(im1, aligned)
    return aligned, error


def align_affine(im1, im2, method="AKAZE"):
    """
    Alignement bas√© sur un mod√®le affine (translation, rotation, √©chelle).
    Compatible ORB / AKAZE / SIFT selon les conditions d'√©clairage.
    """
    im1_gray = normalize_brightness(im1)
    im2_gray = normalize_brightness(im2)

    # Choix du d√©tecteur
    if method == "AKAZE":
        detector = cv2.AKAZE_create()
    elif method == "SIFT":
        detector = cv2.SIFT_create()
    else:
        detector = cv2.ORB_create(max_features)

    keypoints1, descriptors1 = detector.detectAndCompute(im1_gray, None)
    keypoints2, descriptors2 = detector.detectAndCompute(im2_gray, None)

    if descriptors1 is None or descriptors2 is None:
        return None, np.inf

    # Choix automatique du matcher
    if method in ["ORB", "AKAZE"]:
        matcher = cv2.DescriptorMatcher_create(cv2.DescriptorMatcher_BRUTEFORCE_HAMMING)
    else:  # SIFT ‚Üí float32
        matcher = cv2.DescriptorMatcher_create(cv2.DescriptorMatcher_BRUTEFORCE)

    # Calcul des correspondances
    matches = matcher.match(descriptors1, descriptors2)
    if isinstance(matches, cv2.DMatch):
        matches = [matches]

    matches = sorted(matches, key=lambda x: x.distance)
    good_matches = matches[:int(len(matches) * good_match_percent)]

    if len(good_matches) < 3:
        return None, np.inf

    pts1 = np.float32([keypoints1[m.queryIdx].pt for m in good_matches])
    pts2 = np.float32([keypoints2[m.trainIdx].pt for m in good_matches])

    # Estimation d'une transformation affine robuste
    M, inliers = cv2.estimateAffinePartial2D(pts2, pts1, method=cv2.RANSAC)
    if M is None:
        return None, np.inf

    height, width, _ = im1.shape
    aligned = cv2.warpAffine(im2, M, (width, height))
    error = alignment_error(im1, aligned)
    return aligned, error


def align_best(output_dir, fname, ref, img):
    methods = [(AlignmentMethod.ALIGN_ORB, "align_orb", align_orb),
               # (AlignmentMethod.ALIGN_ECC, "align_ecc", align_ecc),
               # (AlignmentMethod.ALIGN_PHASE_CORRELATION, "align_phase_correlation", align_phase_correlation),
               (AlignmentMethod.ALIGN_AFFINE_AKAZE, "align_affine_akaze", lambda r, i: align_affine(r, i, "AKAZE")),
               (AlignmentMethod.ALIGN_AFFINE_SIFT, "align_affine_sift", lambda r, i: align_affine(r, i, "SIFT")),
               (AlignmentMethod.ALIGN_AFFINE_ORB, "align_affine_orb", lambda r, i: align_affine(r, i, "ORB")),
            ]


    # ("ORB + homography", align_orb),
    #     ("ECC affine", align_ecc),
    #     ("Phase correlation", align_phase_correlation),
    #     ("AKAZE affine", lambda r, i: align_affine(r, i, "AKAZE")),
    #     ("SIFT affine", lambda r, i: align_affine(r, i, "SIFT")),
    # ]

    best_img, best_error = None, np.inf
    best_method = None
    for method, method_name, method_function in methods:
        aligned, error = method_function(ref, img)

        if aligned is None:
            print(f"‚ö†Ô∏è       Tentative rat√©e pour {fname} avec {method_name}")

        if aligned is not None and DEBUG_ALL_IMAGES:
            # Backup image for checking result
            cv2.imwrite(add_suffix(output_dir, PREFIX_DEBUG_IMAGES + fname, '-' + method_name), aligned)

        if aligned is not None and error < best_error:
            best_img, best_error = aligned, error
            best_method = method

    if best_error > ALIGNMENT_THRESHOLD:
        return None, None, AlignmentMethod.ERROR, best_error
    else:
        if DEBUG_METHODS:
            print(f"       {fname} : {best_method} : {best_error}")
        return best_img, None, best_method, best_error




# def normalize_log_gray(img):
#     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
#     gray = gray.astype(np.float32) / 255.0
#     gray = np.log1p(gray * 9.0)  # compression dynamique
#     gray = cv2.normalize(gray, None, 0, 255, cv2.NORM_MINMAX)
#     return gray.astype(np.uint8)
#
# def preprocess_for_alignment(img):
#     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
#     gray = cv2.GaussianBlur(gray, (3,3), 0)
#     gx = cv2.Sobel(gray, cv2.CV_32F, 1, 0, ksize=3)
#     gy = cv2.Sobel(gray, cv2.CV_32F, 0, 1, ksize=3)
#     mag = cv2.magnitude(gx, gy)
#     mag = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)
#     return mag.astype(np.uint8)


# def reprojection_error(H, src_pts, dst_pts, affine=False):
#     if affine:
#         # src_pts: Nx2
#         src_pts_h = np.hstack([src_pts, np.ones((len(src_pts), 1))])
#         pred = (H @ src_pts_h.T).T  # Nx2
#     else:
#         src_pts_h = np.hstack([src_pts, np.ones((len(src_pts), 1))])
#         pred_h = (H @ src_pts_h.T).T  # Nx3
#         pred = pred_h[:, :2] / pred_h[:, 2:3]
#
#     err = np.linalg.norm(pred - dst_pts, axis=1)
#     return np.median(err)  # ou np.mean(err)



def ajouter_date_image(chemin_image):
    # --- 1. Extraire le nom de fichier sans extension ---
    nom_fichier = os.path.basename(chemin_image)

    # --- 2. Trouver la date au format yyyy-mm-dd ---
    match = re.search(r"\d{4}-\d{2}-\d{2}", nom_fichier)
    if not match:
        print("Aucune date trouv√©e dans le nom de fichier :", nom_fichier)
        return
    date_str = match.group(0)

   # --- 3. Charger l‚Äôimage et forcer RGBA ---
    img = Image.open(chemin_image)
    if img.mode != "RGBA":
        img = img.convert("RGBA")

    # --- 4. Police monospace ---
    try:
        font = ImageFont.truetype("DejaVuSansMono.ttf", size=max(20, img.height // 25))
    except IOError:
        font = ImageFont.load_default()

    # --- 5. Mesure du texte ---
    draw_tmp = ImageDraw.Draw(img)
    bbox = draw_tmp.textbbox((0, 0), date_str, font=font)
    text_width = bbox[2] - bbox[0]
    text_height = bbox[3] - bbox[1]

    # --- 6. Position bas droite ---
    margin = 10
    x = img.width - text_width - margin
    y = img.height - text_height - margin

    # --- 7. Cr√©er calque overlay RGBA ---
    overlay = Image.new("RGBA", img.size, (0, 0, 0, 0))
    draw_overlay = ImageDraw.Draw(overlay)

    # Fond semi-transparent
    draw_overlay.rectangle(
        [(x - 5, y - 3), (x + text_width + 5, y + text_height + 3)],
        fill=(0, 0, 0, 150)
    )
    # Texte jaune vif
    draw_overlay.text((x, y), date_str, font=font, fill=(255, 255, 0, 255))

    # --- 8. Fusion des calques ---
    img_composite = Image.alpha_composite(img, overlay)

    # --- 9. Conversion finale en RGB pour JPEG ---
    img_final = img_composite.convert("RGB")
    # sortie = os.path.splitext(chemin_image)[0] + "_date.jpg"
    # On √©crase !
    sortie = chemin_image
    img_final.save(sortie, quality=95)
    # print("‚úÖ Image enregistr√©e :", sortie)


# BEGIN OLD VERSION
# # TODO: detect aberrations in case none of the 2 alignment methods are accurate enough
#
# def align_images(base_img, img_to_align, fname, output_dir):
#     """Aligne img_to_align sur base_img via d√©tection de points cl√©s (ORB) et homographie."""
#     # Convertir en niveaux de gris
#
#     # im1_gray = cv2.cvtColor(base_img, cv2.COLOR_BGR2GRAY)
#     # im2_gray = cv2.cvtColor(img_to_align, cv2.COLOR_BGR2GRAY)
#
#     # Mieux ?
#     # im1_gray = cv2.equalizeHist(cv2.cvtColor(base_img, cv2.COLOR_BGR2GRAY))
#     im2_gray = cv2.equalizeHist(cv2.cvtColor(img_to_align, cv2.COLOR_BGR2GRAY))
# #
#     clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
#     im1_gray = clahe.apply(cv2.cvtColor(base_img, cv2.COLOR_BGR2GRAY))
#     im2_gray = clahe.apply(cv2.cvtColor(img_to_align, cv2.COLOR_BGR2GRAY))
#
#     # im1_gray = normalize_log_gray(base_img)
#     # im2_gray = normalize_log_gray(img_to_align)
#
#     # im1_gray = preprocess_for_alignment(base_img)
#     # im2_gray = preprocess_for_alignment(img_to_align)
#
#     # D√©tection des points cl√©s et descripteurs
#     # BEGIN ORB
#     # orb = cv2.ORB_create(max_features)
#     # keypoints1, descriptors1 = orb.detectAndCompute(im1_gray, None)
#     # keypoints2, descriptors2 = orb.detectAndCompute(im2_gray, None)
#     # # Appariement des descripteurs
#     # matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)
#     # END ORB
#     # BEGIN AKAZE
#     # detector = cv2.AKAZE_create()  # ou cv2.SIFT_create()
#     # keypoints1, descriptors1 = detector.detectAndCompute(im1_gray, None)
#     # keypoints2, descriptors2 = detector.detectAndCompute(im2_gray, None)
#     #
#     # # Appariement des descripteurs
#     # matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)
#     # END AKAZE
#
#     detector = cv2.SIFT_create()
#     keypoints1, descriptors1 = detector.detectAndCompute(im1_gray, None)
#     keypoints2, descriptors2 = detector.detectAndCompute(im2_gray, None)
#
#     # Appariement des descripteurs
#     matcher = cv2.DescriptorMatcher_create(cv2.DescriptorMatcher_FLANNBASED)
#
#     # BEGIN SIMPLE COMPARISON
#     # # matches = matcher.match(descriptors1, descriptors2, None)
#     # matches = list(matcher.match(descriptors1, descriptors2))
#     #
#     # # Trier selon la qualit√© (distance)
#     # matches.sort(key=lambda x: x.distance, reverse=False)
#     #
#     # # Garder les meilleures correspondances
#     # num_good_matches = int(len(matches) * good_match_percent)
#     # matches = matches[:num_good_matches]
#     # END SIMPLE COMPARISON
#
#     # Appariement avec knn
#     potential_matches = matcher.knnMatch(descriptors1, descriptors2, k=2)
#
#     matches = []
#     for m, n in potential_matches:
#         if m.distance < 0.75 * n.distance:
#             matches.append(m)
#
#     # Extraire les points correspondants
#     points1 = np.zeros((len(matches), 2), dtype=np.float32)
#     points2 = np.zeros((len(matches), 2), dtype=np.float32)
#     for i, match in enumerate(matches):
#         points1[i, :] = keypoints1[match.queryIdx].pt
#         points2[i, :] = keypoints2[match.trainIdx].pt
#
#     height, width, channels = base_img.shape
#
#     # Calcul de la transformation par homographie
#     h_homog, mask_h = cv2.findHomography(points2, points1, cv2.RANSAC)
#
#     # Calcul de la transformation affine
#     h_affine, mask_a = cv2.estimateAffinePartial2D(points2, points1, method=cv2.RANSAC)
#
#     err_homog = reprojection_error(h_homog, points2, points1, affine=False)
#     err_affine = reprojection_error(h_affine, points2, points1, affine=True)
#
#     method = AlignmentMethod.HOMOGRAPHY
#     h = h_affine
#     deviation = -1
#
#     if err_affine > ERROR_THRESHOLD and err_homog > ERROR_THRESHOLD:
#         use_affine = True
#         method = AlignmentMethod.FALLBACK_AFFINE
#         print(f"‚ö†Ô∏è Valeur aberrante d√©tect√©e : err_affine = {err_affine} ; err_homog = {err_homog}")
#     else:
#         if err_affine < err_homog:
#             use_affine = True
#             method = AlignmentMethod.AFFINE
#         else:
#             use_affine = False
#             method = AlignmentMethod.HOMOGRAPHY
#
#     # if h is None or np.linalg.cond(h) > 1e4:
#     #     h, mask = cv2.estimateAffinePartial2D(points2, points1, method=cv2.RANSAC)
#     #     if h is not None:
#     #         print(f"‚ö†Ô∏è Homographie douteuse pour {fname}, backup vers translation.")
#     #         aligned_image = cv2.warpAffine(img_to_align, h, (width, height))
#     #     else:
#     #         print(f"‚ö†Ô∏è Homographie et translation douteuses pour {fname}, copie non align√©e.")
#     #         aligned_image = img_to_align.copy()
#     # else:
#     #     # Appliquer la transformation homographie
#     #     aligned_image = cv2.warpPerspective(img_to_align, h, (width, height))
#
#
#     if use_affine:
#         # Appliquer la transformation affine
#         aligned_image = cv2.warpAffine(img_to_align, h_affine, (width, height))
#         h = h_affine
#         deviation = err_affine
#
#         if DEBUG_ALL_IMAGES:
#             # Generate the method using the other less good method
#             bad_aligned_image = cv2.warpPerspective(img_to_align, h_homog, (width, height))
#             cv2.imwrite(add_suffix(output_dir, fname, '-homography-BAD'), bad_aligned_image)
#     else:
#         # Appliquer la transformation homographie
#         aligned_image = cv2.warpPerspective(img_to_align, h_homog, (width, height))
#         h = h_homog
#         deviation = err_homog
#         # print(f"‚ö†Ô∏è Meilleure homographie pour {fname}.")
#         if DEBUG_ALL_IMAGES:
#             # Generate the method using the other less good method
#             bad_aligned_image = cv2.warpAffine(img_to_align, h_affine, (width, height))
#             cv2.imwrite(add_suffix(output_dir, fname, '-affine-BAD'), bad_aligned_image)
#
#     return aligned_image, h, method, deviation
# END OLD VERSION

def align_folder(input_dir, output_dir):
    os.makedirs(output_dir, exist_ok=True)
    filenames = sorted([
        f for f in os.listdir(input_dir)
        if f.lower().endswith((".jpg", ".jpeg", ".png", ".tif"))
    ])

    if not filenames:
        print("Aucune image trouv√©e dans", input_dir)
        return

    # Image de r√©f√©rence
    ref_path = os.path.join(input_dir, filenames[0])
    ref_img = cv2.imread(ref_path)
    if ref_img is None:
        raise ValueError(f"Impossible de lire l'image de r√©f√©rence : {ref_path}")

    print(f"Image de r√©f√©rence : {filenames[0]}")

    # Sauvegarder l'image de r√©f√©rence telle quelle
    # TODO: mieux de d'abord cr√©er l'image puis ajouter le texte, et enfin l'exporter dans un fichier
    cv2.imwrite(os.path.join(output_dir, filenames[0]), ref_img)
    ajouter_date_image(os.path.join(output_dir, filenames[0]))

    counter_methods = Counter()
    total_deviation = 0
    nb_non_errors = 0

    # Aligner les autres
    for fname in tqdm(filenames[1:], desc="Alignement des images"):
        path = os.path.join(input_dir, fname)
        img = cv2.imread(path)
        if img is None:
            print(f"‚ö†Ô∏è  Impossible de lire {fname}, ignor√©e.")
            continue
        # aligned, _, method, deviation = align_images(ref_img, img, fname, output_dir)
        aligned, _, method, deviation = align_best(output_dir, fname, ref_img, img)

        if aligned is None:
            print(f"‚ö†Ô∏è Aucun alignement pour {fname} ! (erreur min : {deviation})")

        counter_methods[method] += 1
        total_deviation += deviation if method != AlignmentMethod.ERROR else 0
        nb_non_errors += 1 if method != AlignmentMethod.ERROR else 0
        if aligned is not None:
            # TODO: mieux de d'abord cr√©er l'image puis ajouter le texte, et enfin l'exporter dans un fichier
            cv2.imwrite(os.path.join(output_dir, fname), aligned)
            ajouter_date_image(os.path.join(output_dir, fname))

    average = total_deviation / nb_non_errors if nb_non_errors > 0 else 0

    for alignment in AlignmentMethod:
        print(f"{alignment.value:<10} : {counter_methods[alignment]:>3}")
    print(f"Average deviation: {average:>3} (for {nb_non_errors} non-errors)")

    print(f"\n‚úÖ Toutes les images ont √©t√© align√©es dans : {output_dir}")

############################################################
# PART 3: CONVERSION TO VIDEO
############################################################

def photos_to_video_ffmpeg(images_dir, output_file):
    print(f"Conversion des photos dans le r√©pertoire '{images_dir}'‚Ä¶")
    # V√©rifie la pr√©sence du dossier
    if not os.path.isdir(images_dir):
        print("‚ùå Le dossier sp√©cifi√© n'existe pas :", images_dir)
        sys.exit(1)

    # V√©rifie que ffmpeg est install√©
    if subprocess.call(["which", "ffmpeg"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL) != 0:
        print("‚ùå ffmpeg n'est pas install√©. Installez-le avec :")
        print("   sudo apt install ffmpeg")
        sys.exit(1)

    # Cr√©e une liste d‚Äôimages tri√©e (pour garantir l‚Äôordre)
    images = sorted([
        f for f in os.listdir(images_dir)
        if f.lower().endswith((".jpg", ".jpeg", ".png", ".bmp")) and not (f.startswith(PREFIX_DEBUG_IMAGES))
    ])

    if not images:
        print("‚ùå Aucune image trouv√©e dans le dossier :", images_dir)
        sys.exit(1)

    # Cr√©e un fichier temporaire listant les images pour ffmpeg
    list_file = os.path.join("images.txt")
    with open(list_file, "w") as f:
        for img_name in images:
            f.write(f"file '{os.path.join(images_dir, img_name)}'\n")
            f.write("duration " + DURATION_FRAME + "\n") # NOTE: incompatible with `-r fps`
        # derni√®re image r√©p√©t√©e
        f.write(f"file '{os.path.join(images_dir, images[-1])}'\n")

    # print("Fichier temporaire : " + list_file)

    cmd = [
        "ffmpeg",
        "-y",
        "-f", "concat",
        "-safe", "0",
        "-i", list_file,
        # "-r", str(fps),
        "-pix_fmt", "yuv420p",
        output_file
    ]

    print(f"üéûÔ∏è Cr√©ation de la vid√©o '{output_file}' √† dur√©e d‚Äôimage {DURATION_FRAME}s‚Ä¶")
    subprocess.run(cmd, check=True)
    os.remove(list_file)

    print(f"‚úÖ Vid√©o cr√©√©e avec succ√®s : {output_file}")

############################################################
# MAIN
############################################################

if __name__ == "__main__":
    # V√©rification des arguments
    if len(sys.argv) < 2:
        print("Usage : python script.py <repertoire_entree> [prefixe] [repertoire_sortie]")
        sys.exit(1)

    repertoire_entree = sys.argv[1]
    prefixe = sys.argv[2] if len(sys.argv) >= 3 else "exif"

    repertoire_sortie_exif = sys.argv[3] if len(sys.argv) >= 4 else repertoire_entree.rstrip("/\\") + "-exif"
    repertoire_sortie_exif_aligne = repertoire_sortie_exif + "-alignes"


    if not os.path.isdir(repertoire_entree):
        print(f"Erreur : le r√©pertoire d‚Äôentr√©e '{repertoire_entree}' n‚Äôexiste pas.")
        sys.exit(1)

    print("\n=== Entr√©es ===")
    print(f"R√©pertoire source                    : {repertoire_entree}")
    print(f"Pr√©fixe                              : {prefixe}")
    print(f"R√©pertoire de sortie EXIF            : {repertoire_sortie_exif}")
    print(f"R√©pertoire de sortie EXIF alignement : {repertoire_sortie_exif_aligne}")

    os.makedirs(repertoire_sortie_exif, exist_ok=True)


    rename_after_exif(repertoire_entree, repertoire_sortie_exif)
    align_folder(repertoire_sortie_exif, repertoire_sortie_exif_aligne)
    photos_to_video_ffmpeg(repertoire_sortie_exif_aligne, VIDEO_NAME)

print("Done! üéûüëã")
